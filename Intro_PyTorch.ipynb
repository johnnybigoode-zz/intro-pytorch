{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introdução ao PyTorch\n",
    "\n",
    "Lucas de Magalhães Araújo   \n",
    "Mestrando do Instituto de Computação da Unicamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Podemos considerar o PyTorch por dois ângulos diferentes:\n",
    "\n",
    " - Como uma ferramenta de diferenciação automática (Autograd);\n",
    " - Como um framework de Machine Learning.\n",
    " \n",
    "Nesta apresentação, veremos estes dois aspectos e como eles se relacionam para realizar experimentos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Importa bibliotecas e define funções auxiliares\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Funções auxiliares de visualização\n",
    "def plot_loss(loss):\n",
    "    sns.set()\n",
    "    plt.plot(loss)\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_and_data(dataset, model, title=''):\n",
    "    if \"Data\" in str(type(dataset)):\n",
    "        print(\"Tamanho dataset:\", len(dataset))\n",
    "        l = list(dataset)\n",
    "        x = [p[0].item() for p in l]\n",
    "        x = torch.tensor(x)\n",
    "        y = [p[1].item() for p in l]\n",
    "        y = torch.tensor(y)\n",
    "    else:\n",
    "        x = dataset[0]\n",
    "        y = dataset[1]\n",
    "    sns.set()\n",
    "    plt.plot(x.tolist(), model(x).tolist(), label='modelo', color='red')\n",
    "    plt.scatter(x.tolist(), y.tolist(), label='dataset', color='green')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset(dataset):\n",
    "    if \"Data\" in str(type(dataset)):\n",
    "        print(\"Tamanho dataset:\", len(dataset))\n",
    "        l = list(dataset)\n",
    "        x = [p[0].item() for p in l]\n",
    "        y = [p[1].item() for p in l]\n",
    "    else:\n",
    "        x = dataset[0].tolist()\n",
    "        y = dataset[1].tolist()\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.scatter(x, y, color='green')\n",
    "    plt.title(\"Dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_sinc(x, y):\n",
    "    print(\"x.grad middle:\", x.grad[n_samples//2-2:n_samples//2+3].tolist())\n",
    "    print(\"y middle:\", y[n_samples//2-2:n_samples//2+3].tolist())\n",
    "    sns.set()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.plot(x.detach().numpy(), y.detach(), label='y')\n",
    "    plt.plot(x.detach(), x.grad, label='dy/dx')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1: Autograd\n",
    "\n",
    "O paradigma de *diferenciação automática* permite computar o gradiente de funções a partir do seu **grafo computacional**. \n",
    "Em PyTorch, operações com tensores formam grafos computacionais dinamicamente. Estes tensores (objetos da classe **Tensors**) são o tipo básico de dados em PyTorch. \n",
    "\n",
    "Tensores possuem dois parâmetros convenientes:\n",
    " - $\\texttt{requires_grad}$, que permite indicar as variáveis que temos interesse em computar o gradiente;\n",
    " - $\\texttt{device}$, que permite alocar o tensor na CPU ou GPU (quando disponível). Desta maneira, a paralelização da computação do grafo é transparente ao usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo: grafo computacional e computação do gradiente por diferenciação automática (backpropagation)\n",
    "\n",
    "Queremos computar $\\dfrac{dy}{dx}$, dado $y = (1.5(2.5 x))^2 = 9.0 x^2$.\n",
    "\n",
    "Quebrando a expressão em\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      x = \\text{algum escalar} & \\\\\n",
    "      m_1 = 2.5 x & \\\\\n",
    "      m_2 = 1.5 m_1 & \\\\\n",
    "      y = m_2^2 & \n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "e aplicando a regra da cadeia, temos\n",
    "\n",
    "$$\n",
    "\\dfrac{dy}{dx} = \\dfrac{dm_1}{dx}.\\dfrac{dy}{dm_1} = \\dfrac{dm_1}{dx}.\\dfrac{dm_2}{dm_1}.\\dfrac{dy}{dm_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computação direta (*forward*) é realizada na própria declaração das expressões. A computação do gradiente é realizada pelo métod $\\texttt{backward()}$ aplicado ao alvo do grafo (neste caso, a variável $y$).\n",
    "\n",
    "\n",
    "O grafo computacional com *forward* e *backward* pode ser ilustrado como\n",
    "\n",
    "![Grafo Computacional](./grafo_computacional.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = 2.0\n",
    "\n",
    "# Tensor que se tem interesse em computar o gradiente\n",
    "x = torch.tensor(xval, requires_grad=True)\n",
    "\n",
    "# Expressões parciais\n",
    "m1 = 2*x\n",
    "m2 = 1.5*m1\n",
    "y = m2**2\n",
    "\n",
    "# Registra função para imprimir valor do gradiente parcial\n",
    "def print_grad(name):\n",
    "    def printer(grad):\n",
    "        print(name, grad.item())\n",
    "    return printer\n",
    "x.register_hook(print_grad('dy/dx = dy/dm1 * dm1/dx = dy/dm1 * 2 ='))\n",
    "m1.register_hook(print_grad('dy/dm1 = dy/dm2 * dm2/dm1 = dy/dm2 * 1.5 = '))\n",
    "m2.register_hook(print_grad('dy/dm2 = dy/dy * dy/dm2 = dy/dy * 2*m2 = '))\n",
    "y.register_hook(print_grad('dy/dy = '))\n",
    "\n",
    "print('x =', x.item())\n",
    "print('m1 = 2*x =', m1.item())\n",
    "print('m2 = 1.5*m1 =', m2.item())\n",
    "print('y = m2**2 =', y.item())\n",
    "print()\n",
    "y.backward()\n",
    "\n",
    "print()\n",
    "print(\"dy/dx (analítico) = 18*x =\", 18*x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo: customizando método backward()\n",
    "\n",
    "Em muitos casos, a regra da cadeia será suficiente para computar o gradiente dos parâmetros (variáveis) de interesse. Este é o caso das Redes Neurais e Redes de Convolução, por exemplo. Porém, em alguns contextos podemos querer especificar explicitamente a computação do gradiente. Vejamos o caso da função **sinc**, cujo gradiente automático no ponto de singularidade não tem o comportamento que gostaríamos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    \\text{sinc}(x) = \n",
    "    \\begin{cases}\n",
    "      1&,  x = 0 & \\\\\n",
    "      \\dfrac{\\text{sen}(x)}{x}&,  x \\ne 0 & \\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1001\n",
    "\n",
    "def sinc(x):\n",
    "    return x.sin()/x\n",
    "\n",
    "x = torch.linspace(-4, 4, n_samples, requires_grad=True)\n",
    "#y = sinc(x)\n",
    "y = Sinc.apply(x)\n",
    "y.backward(torch.ones_like(x))\n",
    "\n",
    "# Obs: plota derivada a partir dos valores em x.grad\n",
    "plot_sinc(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customizando gradiente**: \n",
    "- extensão da classe $\\texttt{torch.autograd.Function}$\n",
    "- implementação dos métodos estáticos $\\texttt{forward()}$ e $\\texttt{backward()}$\n",
    "- estes métodos **não são acessados pelo usuário** mas pelo PyTorch durante a computação do grafo. Usuário computa a operação através do método $\\texttt{apply()}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sinc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(context, input_tensor):\n",
    "        \"\"\"\n",
    "        Na passada pelo forward(), recebemos tensor de entrada e devemos\n",
    "        retornar o resultado da operação.\n",
    "        O tensor de entrada (e outras variáveis) podem ser salvas no objeto\n",
    "        'contexto' para ser utilizado no backward().\n",
    "        \"\"\"\n",
    "        sinc = input_tensor.sin() / input_tensor\n",
    "        if sinc.ndimension() == 0:\n",
    "            sinc = sinc.expand(1)\n",
    "        sinc[torch.isnan(sinc)] = 1\n",
    "        context.save_for_backward(input_tensor, sinc)\n",
    "        return sinc.view(input_tensor.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient \n",
    "        with respect to the output, and we need to compute the current gradient \n",
    "        with respect to the output.\n",
    "        \"\"\"\n",
    "        input_tensor, sinc = ctx.saved_tensors\n",
    "        grad = (input_tensor.cos() / input_tensor) - (sinc / input_tensor)\n",
    "        grad[torch.isnan(grad)] = 0\n",
    "        grad = grad.view(input_tensor.shape)\n",
    "        return grad * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear \"na unha\"\n",
    "\n",
    "Vamos utilizar este maquinário de diferenciação automática para computar regressão linear implementando a descida de gradiente \"manualmente\" (sem otimizador).\n",
    "\n",
    "Ou seja, dado um conjunto de dados que possui distribuição linear, queremos descobrir os parâmetros $a, b$ de um modelo $m$ tal que \n",
    "\n",
    "$m = a x + b$\n",
    "\n",
    "minimiza o erro entre a reta e as amostras do conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relembrando: Stochastic Gradient Descent (genérico)\n",
    "\n",
    "**Entrada**:\n",
    " - conjunto de treino $(X,Y)$;\n",
    " - conjunto de pesos $W$ que parametriza um modelo $h$;\n",
    " - taxa de aprendizagem $\\lambda$;\n",
    " - tamanho do batch $m$.\n",
    " \n",
    "**Saída**:\n",
    " - pesos atualizados.\n",
    " \n",
    "**Algoritmo**:\n",
    "\n",
    "**enquanto** não atingir critério de parada:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;    extrair batch com $m$ amostras $((x_1,y_1), …, (x_m,y_m))$ de $(X, Y)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;    computar função de custo $J(W)=\\frac{1}{m}\\sum_{i=1}^md(h_W(x_i),y_i)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;    **para cada** $w \\in W$:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $w \\leftarrow w - \\lambda \\frac{\\partial J}{\\partial w}$\n",
    "\n",
    "**retorna** pesos atualizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria Dataset\n",
    "a_true, b_true = 3, -2\n",
    "n_samples = 100\n",
    "\n",
    "def linear_model(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "x_data = torch.linspace(-5, 5, n_samples)\n",
    "y_data = linear_model(x_data, a_true, b_true) + torch.tensor(np.random.normal(scale=1, size=n_samples))\n",
    "\n",
    "# Plota pontos\n",
    "plot_dataset([x_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "n_epochs = 500\n",
    "\n",
    "# Inicialização aleatória dos pesos \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(\"Pesos iniciais: a={:.4f}, b={:.4f}\".format(a.item(),b.item()))\n",
    "\n",
    "# Ajuste de curva\n",
    "train_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Predição com 'a' e 'b' atuais\n",
    "    y_pred = linear_model(x_data, a, b)\n",
    "\n",
    "    # Loss: MSE\n",
    "    loss = ((y_data - y_pred)**2).mean()\n",
    "    train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    # Computa gradientes \n",
    "    loss.backward()\n",
    "\n",
    "    # Descida de gradiente\n",
    "    # Utilizamos no_grad para não recomputar gradientes\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # Por padrão, o gradiente do PyTorch é cumulativo e cada otimizador\n",
    "    # se encarrega de zerá-lo no momento correto.\n",
    "    # Aqui, temos que fazê-lo manualmente.\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Visualiza treino a cada 50 épocas\n",
    "    if epoch%50 == 0:\n",
    "        print(\"Época {}: loss={:.4f}\".format(epoch, train_loss[epoch]))\n",
    "\n",
    "print(\"Pesos finais: a={:.4f}, b={:.4f}, loss={:.4f}\".format(a.item(), b.item(), train_loss[-1]))\n",
    "print(\"Queríamos encontrar: a={:.4f}, b={:.4f}\".format(a_true, b_true))\n",
    "\n",
    "plot_loss(train_loss)\n",
    "plot_model_and_data([x_data, y_data], lambda x: linear_model(x, a, b), title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Framework de ML\n",
    "\n",
    "Além do mecanismo de diferenciação automática, PyTorch é um framework para expressar problemas de Machine Learning. A maneira típica de trabalho é expressar o problema em três partes:\n",
    "\n",
    " - Representação do conjunto de dados do problema como implementação da classe $\\texttt{Dataset}$;\n",
    " - Representação do model que irá fazer as predições como implementação da classe $\\texttt{Model}$;\n",
    " - Loop de treinamento, que otimiza os parâmetros do modelo utilizando otimizador e função de custo.\n",
    " \n",
    "Vejamos a seguir cada componente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Dataset\n",
    "\n",
    "Datasets em PyTorchs devem ser criados como sub-classe da classe abstrata $\\texttt{torch.utils.data.Dataset}$. Dois métodos precisam ser implementados:\n",
    "\n",
    " - $\\texttt{__getitem__(index)}$: retorna a amostra de índice $\\texttt{index}$. Tipicamente, os conjuntos de dados tem amostras na forma (entrada, alvo). Este método permite acessar amostras pelo uso de índice entre colchetes: $\\texttt{dataset[index]}$.\n",
    " - $\\texttt{__len__()}$: retorna o tamanho do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso dos métodos __getitem__ e __len___\n",
    "l = [-3, 5, 0, 4, 22]\n",
    "\n",
    "print(\"l[2]:\", l[2])\n",
    "print(\"l.__getitem__(2):\", l.__getitem__(2))\n",
    "print(\"len(l):\", len(l))\n",
    "print(\"l.__len__():\", l.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './dataset' | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo amostra\n",
    "!cat './dataset/0233.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class LinearData(Dataset):\n",
    "    def __init__(self, dataset_dir='./dataset'):\n",
    "        self.files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir)]\n",
    "        self.files.sort()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point = np.loadtxt(self.files[idx])\n",
    "        x, y = torch.tensor(point)\n",
    "        return x,y \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "dataset = LinearData()\n",
    "\n",
    "plot_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Model\n",
    "\n",
    "Modelos em PyTorch devem ser criados como sub-classe da classe abstrata $\\texttt{torch.nn.Module}$. \n",
    "\n",
    " - Devemos implementar o método $\\texttt{forward(input)}$, que deve retornar o resultado da computação do modelo com a entrada $\\texttt{input}$\n",
    " - Convém que os parâmetros do modelo sejam instanciados como objetos da classe $\\texttt{torch.nn.Parameter}$. Estes parâmetros serão os alvos da otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "# Obs: já existe o modelo Linear disponível,\n",
    "# aqui estamos reimplementando por fins didáticos\n",
    "class LinearModel(Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.a = Parameter(torch.randn(1, requires_grad=True))\n",
    "        self.b = Parameter(torch.randn(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.a*x + self.b\n",
    "\n",
    "model = LinearModel()\n",
    "plot_model_and_data(dataset, model, title='Modelo Pré-Treino')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop básico de treinamento\n",
    "\n",
    "Instanciados o dataset e modelo, o ciclo básico de treinamento segue a estrutura ilustrada abaixo. \n",
    "\n",
    "    # A classe DataLoader automaticamente extrai batches do dataset\n",
    "    dataloader = DataLoarder(dataset, batch_size)\n",
    "    \n",
    "    # Instancia otimizador, inicializado com learning rate, \n",
    "    # parâmetros do modelo e outros parâmetros específicos\n",
    "    # de cada otimizador.\n",
    "    optimizer = Optimizer(model.parameters(), lr)\n",
    "    \n",
    "    # Para cada época\n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # Para cada época\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Zera os gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predição do modelo atual\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            # Erro da predição em relação ao alvo\n",
    "            loss = loss_function(predictions, targets)\n",
    "            \n",
    "            # Computa o gradiente dos parâmetros\n",
    "            loss.backward()\n",
    "            \n",
    "            # Atualiza os parâmetros do modelo\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear com Framework PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lr = 5e-3\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Cola\n",
    "a_true = -4\n",
    "b_true = np.pi\n",
    "\n",
    "# Loader: se encarrega em criar batches\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Os parâmetros do modelo são as variáveis que o otimizador\n",
    "# quer otimizar\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# Imprime pesos iniciais\n",
    "a = model.state_dict()['a']\n",
    "b = model.state_dict()['b']\n",
    "print(\"Pesos iniciais: a={:.4f}, b={:.4f}\".format(a.item(),b.item()))\n",
    "print()\n",
    "\n",
    "# Ajuste de curva\n",
    "loss_history = []\n",
    "for i in range(epochs):\n",
    "    # Ajuste por mini-batches \n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch, y_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(loader)\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(\"\\rÉpoca {} de {}. Loss: {:.4f}\".format(i+1, epochs, loss_history[-1]), end='')\n",
    "print()\n",
    "\n",
    "# Plota estado final e curva de treino\n",
    "a = model.state_dict()['a']\n",
    "b = model.state_dict()['b']\n",
    "print(\"Pesos finais: a={:.4f}, b={:.4f}. Loss={:.4f}\".format(a.item(), b.item(), loss_history[-1]))\n",
    "print(\"Queríamos encontrar: a={:.4f}, b={:.4f}\".format(a_true, b_true))\n",
    "plot_model_and_data(dataset, model, title='Modelo Pré-Treino')\n",
    "plot_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss esperada a partir da distribuição original\n",
    "x_data = torch.tensor([p[0] for p in dataset])\n",
    "y_data = torch.tensor([p[1] for p in dataset])\n",
    "\n",
    "y_pred = linear_model(x_data, a_true, b_true)\n",
    "loss = loss_fn(y_data, y_pred)\n",
    "print(\"Loss esperada: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "**Autograd**:\n",
    " - https://pytorch.org/docs/stable/notes/autograd.html\n",
    " - https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "**Dataset**:\n",
    " - https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    " \n",
    "**Module**:\n",
    " - https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    " \n",
    "**Tutorial**:\n",
    " - https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
